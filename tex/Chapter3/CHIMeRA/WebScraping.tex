\documentclass{standalone}

\begin{document}

\section[Web Scraping]{How to find the data - Web Scraping}\label{chimera:web_scraping}

The INFN \href{https://agenda.infn.it/event/16961/contributions/34949/attachments/24579/28029/filoblu_0312.pdf}{FiloBlu project} was developed by the collaboration between the Physics Department of the University of Bologna and the INFN group of the Sapienza University of Rome.
The project aims to implement a NLP pipeline to process messages with medical theme, helping doctor-patient interactions.
Domiciliary care for oncology patients is preferred due to cheaper costs than hospitalization, and a more comfortable living for them.
To successfully follow therapies during domiciliary care, the patient is in constant contact with health-care professionals and he is frequently monitored.
Patients are interested in an actively collaboration to the management of their health and they are willing to use also ICT technologies.
The FiloBlu project meets the citizens' needs developing a tool to optimize the efficiency and the effectiveness of care processes, developing two APPs (patient and medical sides) to support doctor-patient communication.
The final purpose of the project is to process doctor-patient chat messages (using an interface similar to \quotes{WhatsApp}), computing from them a score related to the patient state.
The APPs are equipped with features specifically designed for health-care applications and using a Natural Language Processing pipeline on the text messages they compute an \quotes{attention} score for each text message.
The \quotes{attention} score is then used to rank the patients' messages on the medical-side APP, prioritizing (potential) critical situations.

FiloBlu was financed by POR-FESR project in Lazio region in collaboration with the Sant'Andrea Hospital of Rome, so the project was developed only for Italian language communications.
This constraint drastically affects the data availability, which are very hard to find on-line.
Text message analysis concerns the evaluation of critical keywords and medical terms, so we faced this problem generating a diseases ontology.
In particular, we are interesting in the relation between symptoms, diseases and their mutual interactions for the realization of our score function.
More details about the pipeline used for the message processing are given in Appendix E - Neural Network as a Service.

The English is becoming the predominant language in the research community and it is really hard to find (enough) data in other languages: everyone who wants to share his data via Internet has to provide them in English if he wants to increase its visibility and availability.
The Italian constraint posed by the project, drastically limits the data sources and no public databases were found.
We would stress that as \quotes{database} we consider a publicly available set of structured data, which can be downloaded and easily used.

Surfing on Internet many web pages can be found about diseases and their interactions with symptoms and causes, the so-called \emph{on-line doctor}\footnote{
  Famous English applications are \href{https://www.steadymd.com/?utm_source=bestonlinedoctors&utm_medium=partner&utm_campaign=bizdev}{SteadyMD}, \href{https://www.mdlive.com/}{MDLIVE}, \href{https://sherpaa.com/}{Sherpaa}, \href{https://livehealthonline.com/}{LiveHealth Online} and so on.
  Each service provides slight different information and the choice of the best one vary according to the user needs.
} (or Medical Services) pages.
An on-line doctor is a querable Internet service which allows user-auto-diagnosis based on the information inserted.
The reliability of the information stored in these tools is only partially guaranteed by the service provider and, thus, it can not be considered as a scientific method for medical diagnosis.
However, the amount of information collected by these applications is very interesting, and it can be used to simulate reasonable medical queries, needed by our project.
Also in this case, it is important to notice that, despite the availability of these public information, the data are structured according to the web page needs and, moreover, there is not an immediate download availability of the raw data.

So, how can we obtain these useful information and re-organize them into a structured data format?
The answer is given by the \textsf{web-scraping} techniques.
With the term \textsf{web-scraping} we identify the wide set of algorithms developed to extract information from a website, or, more in general, from the Internet: while \textsf{web-scraping} can be done also manually, with this term we typically refer to automated methods.
All the Internet pages are intrinsically pieces of codes written in different programming languages (\textsf{HTML}, \textsf{PHP}, $\cdot$).
The major part of websites are written in \textsf{HTML}, an extreme verbose language, with more or less \textsf{JavaScript} supports.
The way chosen to write a code and to reach the desired output is always left to the programmer: in these way we do not have a rigid standard (excepted by the programming language constraints) and in each website underlies a potential completely different ensemble of code lines.
Thus, the realization of a web-scraper poses several issues to the programmer, who has to find underlying patterns inside the web page to get the information stored.
In other words, the \textsf{web-scraping} technique is an emblematic example of Big Data Analytics algorithm, since it aims to extract a \emph{value} from a large amount of \emph{unstructured} information (raw website code).

A \textsf{web-scraping} algorithm is made by a series of multiple steps, which have to be performed automatically (without human overview).
First of all, the algorithm has to recognize unique website structures: we can broadly summarize this task as the parsing of the underlying \textsf{HTML} code.
Inside the large amount of code lines\footnote{
  Very large if we consider a pure \textsf{HTML} web page.
} are stored the useful information for our application.
So, the algorithm should be able to detect relevant and interesting parts and filter them.
Then it can easily reorganize the information into a usable data format and save the result.

There are multiple ways in which all these tasks could be addressed, and multiple open source libraries provide user friendly interfaces for the creation of own web-scraper.
The most common one (and also used in our applications) is the \textsf{BeautifulSoup}~\cite{richardson2007beautiful} \textsf{Python} package.
This package provides a very powerful \textsf{Python} library designed to navigate and read website source codes.
The integration of this library with other pre- and post-processing techniques allows the extraction of the desired information from a website and, moreover, their reorganization into a structured data format.

\end{document}
