\documentclass{standalone}

\begin{document}

\chapter*{Appendix A - Discriminant Analysis}

The classification problems aim to associate a set of \emph{pattern} to one or more \emph{classes}.
With \emph{pattern} we identify a multidimensional array of data labeled by a pre-determined tag.
In this case we talk about \emph{supervised learning}, i.e the full set of data is already annotated and we have prior knowledge about data association to the belonging classes.
Since in this work only supervised learning algorithms have been analyzed we do not cite other different learning methods.

In machine learning a key rule is assumed by Bayesian methods, i.e methods which use a Bayesian statistical approach to the analysis of data distributions.
It can be proof that if the distributions under analysis are known, i.e a sufficient number of moments of it is known with a sufficient precision, the Bayesian approach is the best possible method to face on the classification problem. % cite!





\section*{Mathematical formulation}

Since the exact knowledge of the prior probabilities and conditional probabilities is possible only on theory a parametric approach is often needed.
A parametric approach aim to create reasonable hypothesis about the distribution under analysis and its fundamental parameters (e.g mean and variance).
In the next of this discussion we focused only on normal distributions for convenience.

Given the multi-dimensional form of Gauss distribution:

$$
G(\mathbf{x}|\mu, \Sigma) = \frac{1}{(2\pi)^{d/2}\cdot\left|\Sigma\right|^{1/2}}\cdot exp\left[-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right]
$$
\\
where $\mathbf{x}$ is a column $d$-dimensional vector, $\mathbf{\mu}$ the mean vector of the distribution, $\Sigma$ the covariance matrix ($d\times d$), $|\Sigma|$ and $\Sigma^{-1}$ the determinant and the inverse of $\Sigma$, respectively, we can notice the $G$ depends quadratically by $\mathbf{x}$,

$$
\Delta^2 = (\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)
$$
\\
where the exponent ($\Delta^2$) is called Mahalanobis distance of vector $\mathbf{x}$ from its mean.
This distance can be reduced to the Euclidean distance when the covariance matrix is the identity $\mathbf{I}$.

The covariance matrix is always symmetric and positive semi-definite (useful information for next algorithmic strategies) so it has an inverse.
If the covariance matrix has only diagonal terms the multidimensional distribution can be express as simple product of $d$ mono-dimensional normal distributions.
In this case the main axes are parallel to the Cartesian axes.

Starting from the multi-variate Gaussian distribution expression\footnote{
  In Machine Learning it will correspond to the conditional probability density.
}, the Bayesian rule for classification problems can be rewrite as:

$$
g_i(\mathbf{x}) = P(w_i|\mathbf{x}) = \frac{p(\mathbf{x}|w_i)P(w_i)}{p(\mathbf{x})} = \frac{1}{(2\pi)^{d/2}\cdot\left|\Sigma_i\right|^{1/2}}\cdot exp\left[-\frac{1}{2}(\mathbf{x}-\mathbf{\mu_i})^T{\Sigma_i}^{-1}(\mathbf{x}-\mathbf{\mu_i})\right] \frac{P(w_i)}{p(\mathbf{x})}
$$
\\
where, removing constant terms ($\pi$ factors and absolute probability density $p(\mathbf{x}) = \sum_{i=1}^s p(\mathbf{x}|w_i)\cdot P(w_i)$) and using the monotonicity of the function, we can extract the logarithmic relation:

$$
g_i(\mathbf{x}) = -\frac{1}{2}(\mathbf{x}-\mu_i)^T{\Sigma_i}^{-1}(\mathbf{x}-\mu_i) -\frac{1}{2}\log\left|\Sigma_i\right|+\log P(w_i)
$$
\\
which is called Quadratic Discriminant function.

The function dependency by the covariance matrix allows 5 different cases:

\begin{itemize}

\item \textbf{$\Sigma_i=\sigma^2I$ - DiagLinear Classifier}

This is the case of completely independence of features, where they have equal variance for each class.
This hypothesis allow us to simplify the discriminant function as:

$$
g_i(\mathbf{x})=-\frac{1}{2\sigma^2}(\mathbf{x^Tx}-2{\mu_i}^T\mathbf{x} + {\mu_i}^T\mu_i) + \log P(w_i)
$$
\\
and removing all the $\mathbf{x^Tx}$ constant terms for each class

$$
g_i(\mathbf{x}) = -\frac{1}{2\sigma^2}(-2{\mu_i}^T\mathbf{x}+{\mu_i}^T\mu_i)+\log P(w_i) = \mathbf{{w_i}^Tx}+\mathbf{w_0}
$$
\\
This simplifications create a linear discriminant function where the separation surfaces between classes are hyper-planes ($g_i(\mathbf{x})=g_j(\mathbf{x})$).

With equal prior probability the function can be rewritten as

$$
g_i(\mathbf{x}) = -\frac{1}{2\sigma^2}(\mathbf{x}-\mu_i)^T(\mathbf{x}-\mu_i)
$$
\\
which is called \emph{nearest mean classifier} where the equal-probability surfaces are hyper-spheres.


\item \textbf{$\Sigma_i = \Sigma$ (diagonal matrix) - Linear Classifier}

In this case the classes have same covariances but each feature has its own different variance.
After the $\Sigma$ substitution in the equation, we obtain

$$
g_i(\mathbf{x}) = -\frac{1}{2}\sum_{k=1}^{s}\frac{(\mathbf{x_k}-\mu_{i,k})^2}{{\sigma_k}^2}-\frac{1}{2}\log\prod_{k=1}^{s}{\sigma_k}^2+\log P(w_i)
$$
\\
where we can remove constant $\mathbf{x_k}^2$ terms (equals for each class) and obtain another time a linear discriminant function where the discriminant surfaces are hyper-planes and equal-probability boundaries given by hyper-ellipsoids.
Note that the only difference from the previous case is the normalization factor of each axes that in this case is given by the its variance.


\item \textbf{$\Sigma_i = \Sigma$ (non-diagonal matrix) - Mahalanobis Classifier}

In this case we assume that each class has the same covariance matrix but they are non-diagonal ones.
The discriminant function becomes

$$
g_i(\mathbf{x}) = -\frac{1}{2}(\mathbf{x}-\mu_i)^T{\Sigma}^{-1}(\mathbf{x}-\mu_i) -\frac{1}{2}\log\left|\Sigma\right|+\log P(w_i)
$$
\\
where we can remove the $\log\left|\Sigma\right|$ term because it is constant for all the classes and we can assume equal prior probability.
In this case we obtain

$$
g_i(\mathbf{x}) = -\frac{1}{2}(\mathbf{x}-\mu_i)^T{\Sigma}^{-1}(\mathbf{x}-\mu_i)
$$
\\
where the quadratic term is the Mahalanobis distance, i.e a normalization of the distance according to the inverse of their covariance matrix.
We can proof that expanding the scalar product and removing the constant term $\mathbf{x^T\Sigma^{-1}x}$, we obtain yet a linear discriminant function with the same properties of the previous case.
In this case the hyper-ellipsoids have axes aligned according to the eigenvectors of the $\Sigma$ matrix.


\item \textbf{$\Sigma_i = {\sigma_i}^2I$ - DiagQuadratic Classifier}

In this case we have different covariance matrix for each class but they are proportional to the identity matrix, i.e diagonal matrix.
The discriminant function in this case becomes

$$
g_i(\mathbf{x}) = -\frac{1}{2}(\mathbf{x}-\mu_i)^T{\sigma_i}^{-2}(\mathbf{x}-\mu_i) -\frac{1}{2}s\log\left|{\sigma_i}^2\right|+\log P(w_i)
$$
\\
where this expression can be further reduced obtaining a quadratic discriminant function.
In this case the equal-probability boundaries are hyper-spheres aligned according to the feature axes.


\item \textbf{$\Sigma_i \neq\Sigma_j$ (general case) - Quadratic Classifier}

Starting from the more general discriminant function we can relabel the variables and highlight its quadratic form as

$$
g_i(\mathbf{x}) = \mathbf{x^TW_{2,i}x}+\mathbf{w_{1,i}^Tx} + \mathbf{w_{0,i}} \quad \mbox{with}\quad \left\{\begin{array}{l} \mathbf{W_{2,i}}=-\frac{1}{2}{\Sigma_i}^{-1}\\ \mathbf{w_{1,i}}={\Sigma_i}^{-1}\mu_i \\ \mathbf{w_{0,i}}=-\frac{1}{2}{\mu_i}^T{\Sigma_i}^{-1}\mu_i-\frac{1}{2}\log\left|\Sigma_i\right|+\log P(w_i) \\ \end{array}\right.
$$
\\
In this case each class has its own covariance matrix $\Sigma_i$ and the equal-probability boundaries are hyper-ellipsoids oriented according to the eigenvectors of the covariance matrix of each class.

\end{itemize}

The Gaussianity of dataset distribution should be tested before using this classifiers.
It can be performed using statistical tests as \emph{Malkovic-Afifi} based on \emph{Kolmogorov-Smirnov} index or just simpler with the empirical visualization of the data points.






\section*{Numerical Implementation}

From a numeric point of view we can exploit each mathematical information and assumption to simplify the computation and improve the numerical stability of our computation.
I would remark that this consideration were taken into account in this work only for the C++ algorithmic implementation since these methods are already implemented in the high-level programming languages as \emph{Python} and \emph{Matlab}\footnote{
  For completeness we have to highlight that for the Matlab case classification functions, i.e \emph{classify}, is already included in the base packages of the software, i.e no external Toolbox are needed, while for the Python case the most common package which implements these techniques are given by the \emph{scikit-learn} library.
  Matlab allows to set the classifier type as input parameter in the function using a simple string which follows the same nomenclature previously proposed.
  Python has a different import for each classifier type: in this case we find correspondence between our nomenclature and the Python one only in \emph{quadratic} and \emph{linear} cases, while the \emph{Mahalanobis} is not considered a putative classifier.
  The \emph{diagquadratic} classifier is called \emph{GaussianNB} (\emph{Naive Bayes Classifier}) instead.
  The last important discrepancy between the two language implementation is in the computation of the variance (and the corresponding covariance matrix): Matlab proposes the variance estimation only in relation to the mean so the normalization coefficient is given by the number of sample except by one ($N-1$), while Python compute the variance with a simple normalization by $N$.
}.

In the previous section we highlight that the covariance matrix is a positive semi-definite and symmetric matrix by definition and this properties allows the matrix inversion.
The computation of the inverse-matrix is a well known complex computation step from a numerical point-of-view and in a general case can be classified as an $O(N^3)$ algorithm.
Moreover the use of a Machine Learning classifier commonly match the use of a cross validation method, i.e multiple subdivision of the dataset in a training and test sets.
This involves the computation of multiple inverse matrix and it could represent the performance bottleneck in many cases (the other computations are quite simple and the algorithm complexity is certainly less than $O(N^3)$).

Using the information about the covariance matrix we can find the best mathematical solution for the inverse matrix computation that in this case is given by the Cholesky decomposition algorithm.
The Cholesky decomposition or Cholesky factorization allows to re-write a positive-definite matrix into the product of two triangular matrix (the first is the conjugate transpose of the second)

$$
\mathbf{A} = \mathbf{LL^T} = \mathbf{U^TU}
$$
\\
The complexity of the algorithm is the same but the inverse estimation is simpler using a triangular matrix and the entire inversion can be performed in-place.
It can also be proof that general inverse matrix algorithms have numerical instability problems compared to the Cholesky decomposition.
In this case the original inverse matrix can be computed by the multiplication of the two inverses as

$$
\mathbf{A^{-1}} = (\mathbf{L^{-1}})^T(\mathbf{L^{-1}}) = (\mathbf{U^{-1}})(\mathbf{U^{-1}})^T
$$
\\
As second bonus, the cross validation methods involve the subdivision of the data in multiple non-independent chunks of the original data.
The extreme case of this algorithm is given by the Leave-One-Out cross validation in which the superposition of the data between folds are $N-1$ (where $N$ is the size of the data).
The statistical influence of the swapped data is quite low and the covariance matrix will be quite similar between one fold to the other (the inverse matrix will be drastically affected from each slight modification of the original matrix instead).
A second step of optimization can be performed computing the original full-covariance matrix of the whole set of data ($O(N^2)$) and at each cross-validation step evaluate the right set of $k$ indexes needed to modify the matrix entrances ($O(N*k)$) that in the Leave-One-Out case are just one.
This second optimization consideration can also be performed in the Diag-Quadratic case substituting the covariance matrix with the simpler variance vector.

% maybe insert some code snippets

Both these two techniques were used in the custom C++ implementation of the Quadratic Discriminant Analysis classifier and in the Diag-Quadratic Discriminant Analysis classifier for the DNetPRO algorithm implementation (see~\ref{DNetPRO}).


\end{document}