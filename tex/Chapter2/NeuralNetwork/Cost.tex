\documentclass{standalone}

\begin{document}

\subsection[Cost function]{Cost function}\label{NN:cost}

A machine learning algorithm is used to minimize or maximize a cost function.
In other words, when we implement a machine learning algorithm we want to know how good is our result according to prior knowledge about the desired results.
So, we have to establish a function ables to represent the error of our model.
This kind of function are commonly called \emph{error functions} or \emph{loss functions} or just simply \emph{cost function}.
In the previous sections, we have shown many algorithms used into a Neural Network model and we have talked about how to update the functional parameters according to the evaluated error.
This error is provided by the cost function.

The cost function represents the final output of our Neural Network model, so it is reasonable to talk about it at the end of this chapter.
There are many kinds of loss functions and there is not a particular one able to works with all kinds of data.
We have to pay attention to chose the right one in our problems.
In particular, we have to take into account the possible presence of outliers, the structure of our model, the computational efficiency of our algorithm and, most of all, the number of classes that we want to predict.
Broadly, we can classify the loss functions into two major categories: the classification losses and the regression losses.
In the first case we want to predict a finite number of categorical values (classes).
In the second case the prediction is performed on a series of continuous values.
Since in this work we are focusing only on classification problems we will only talk about the first case.

The most common cost function is given by the \emph{Mean Square Error} (MSE) or \emph{L2 loss} (very closed to the regularization function hinted at the end of \ref{NN:batchnorm}).
Its mathematical formulation is quite simple and it is given by

$$
MSE = \frac{\sum_{i=1}^{N}\left( y - t \right)^2}{N}
$$
\\
where we follow the nomenclature given in \ref{NN:perceptron} and $N$ is the number of outputs, which is equivalent to the number of classes.
It is one of the most used cost function due to its simplicity either from a mathematical and numerical point-of-view.
The possible output values range from 0 to $\infty$.
With MSE function, the predictions which are far away from actual values are heavily penalized, due to the squaring.

A slight different function is given by the \emph{Mean Absolute Error} (MAE) or \emph{L1 loss} in which we replace the squaring with a module of the error.

$$
MAE = \frac{\sum_{i=1}^{N}|\left( y - t \right)|}{N}
$$

With MAE we loose the information about the error direction (preserved by the squaring in MSE) and just simply evaluate the absolute value of it.

The main differences between these two functions can be summarized as follow: using the MSE function we can easily solve the problem but the MAE function is more robust against possible outliers.
Despite both functions reach the minimum in a perfect classification configuration (error equal to zero), in presence of outliers we have to manage with large differences in the numerator of the function.
With large differences, the square values are greater than the absolute values, but while the MSE tries to adjust its performance to minimize those cases, the other samples pay the higher cost.

A problem related to the MAE function arises during the gradient evaluation.
Its gradient, in fact, is the same throughout, which means that we will have large gradient values also with small differences which is a worse configuration during training.
A simple possible workaround is given by the introduction of a shrinking parameter, given by a dynamic learning rate, when we move closer to the minimum.

When we have to manage multi-class problems there are other common cost functions based on likelihood scores.
The simpler one is the \emph{Cross Entropy loss} or \emph{Log loss}:

$$
CrossEntropyLoss = -(y\cdot\log(t) + (1 - y)\cdot\log(1 - t))
$$

This function just multiply the log of the actual predicted probability by the ground truth class.
In this way, when we have two classes (e.g $t \in [0, 1]$), we can alternatively nullify the two parts of the function\footnote{
  When the actual label is equal to $1$, i.e $y=1$, the second half of the Log Loss function disappears whereas in case of actual label is equal to $0$ the first half is null.
}.
In this way, the loss function heavily penalizes the predictions that are confident but wrong.
This function works with binary classification problems where the output classes are binned in $[0, 1]$.
For this reason the output of the model must be constrained into the $[0, 1]$ domain and thus a proper activation function should be provided.
Classically this loss function is used jointly to the sigmoid activation (ref.~\ref{NN:activation}) which constrains the output of the model in the desired interval.
For this reason in our implementation of the algorithm we have chosen to merge the sigmoid function and the Log Loss function into a single object\footnote{
  We also try to prevent wrong uses of this loss function for laypersons.
  This implementation was already suggested by the \textsf{darknet} library so we simply propagate it in our implementations.
}.

A last duty to mention loss function is the extension of the Log loss to multiple classes, the so-called \emph{Categorical Cross Entropy Loss}.

$$
CategoricalCrossEntropyLoss = -\sum{i=1}^{N}\left( y\cdot\log(t) \right)
$$

This function generalized the previous one for multiple-classes, i.e for problems where the correct output can be only one.
The loss compare the distribution of the predictions, i.e output of the model, with the prior known distribution.
In this way only the probability of the true class will be $1$ and all the other classes will be set to $0$.
Also in this case we have to pay attention to the output of our model which is intended as a probability value ranging in $[0, 1]$.
In particular, this function commonly works jointly to a softmax activation function.
As in the previous case we have chosen to implement this loss function in a separated object associated to the softmax transformation.

Many other loss functions can be mentioned to overcome different kind of problems.
The list of presented loss function is related to the implementation of the \textsf{darknet}-like libraries which are ported also into the \textsf{NumPyNet} and \textsf{Byron} libraries, i.e either in \textsf{Python} and \textsf{C++}.
\textsf{NumPyNet} and \textsf{Byron} libraries provide an optimize version of this function (fixing also some \textsf{darknet} issues) and they include also other kind of loss functions to improve the library usability.
A full list of available loss functions can be found in the \href{https://github.com/Nico-Curti/Byron/blob/master/src/cost_layer.cpp}{on-line} version of the libraries with a list of easily visual examples.

A further improvement has been performed from a numerical point-of-view: many mathematical formulas need expensive math operations as logarithms and trigonometric functions.
An efficient (but approximated) math formula has been implemented both in \textsf{C++} and \textsf{Python} to reach faster computational performances.
These numerical math operations are widely used into the \textsf{Byron} library to increase computational performances, despite their used can be turned off by user at compile time.
The full set of functions, in fact, is enclosed into a \textsf{macro} definition (\textsf{\_\_fmath\_\_}) that can be enabled at compile-time.

A classical example of this faster math operation is given by the \emph{fast inverse square root} algorithm, firstly introduced in 1999 in the source code of \emph{Quake III Arena}, a first-person shooter video game.
The method is based on a Newton algorithm, which can be stopped at the desired precision order: less precision is associated to faster execution, obviously.
In our \textsf{fast math} implementation we provide a set of Newton algorithms related to the most common mathematical operations, like \textsf{exp}, \textsf{log}, \textsf{sqrt} and so on.
We tested these implementations against the common standards (\textsf{Numpy} package for \textsf{Python} and \textsf{std::} for \textsf{C++}) and we compared their execution-time (we required a precision of at least $10^-4$).
The obtained results are shown in Fig.~\ref{fig:fmath}, where we normalized the execution-time, keeping \textsf{Numpy} implementation as reference.

\begin{figure}[htbp]
\centering
\def\svgwidth{0.8\textwidth}
\input{./img/fmath_timing.pdf_tex}
\caption{Time performances of standard mathematical operations implemented using Newton approximations.
We compare the results obtained with the \textsf{Numpy} library (blue, reference) and the standard \textsf{C++} library (\textsf{CMath}, green) to the respective functions implemented in our \textsf{PyFMath} (orange) and \textsf{FMath} (red).
In the comparison we have to keeping in mind that the \textsf{Numpy} library is based on a \textsf{C++} wrap and that the \textsf{Python} version of the \textsf{FMath} is written in pure \textsf{Python} language.
In all the cases the \textsf{FMath} version of the functions performs better or at-least-equal to the standard one.
}
\label{fig:fmath}
\end{figure}

As can be seen, all the results obtained by the \textsf{fast math} algorithms are faster or at least equals to the standard ones.
The \textsf{C++} version of the \textsf{fast math} is certainly the better choice for an optimized implementation of the algorithms in all the cases.
It is interesting to notice how some functions (\textsf{pow2} and \textsf{log10}) are drastically slower in \textsf{C++} than in \textsf{Python}, despite the intrinsic overhead of the \textsf{Python} language.
This is probably due to particular optimizations performed by the \textsf{Numpy} package in the implementation of these special cases: if we compare those functions to the general ones (\textsf{pow} and \textsf{log}), in fact, the results confirm the efficiency of the \textsf{C++} language.

These results highlight the importance of code testing before release it: we have to pay always attention in writing a code and query also the standard choices.


\end{document}
