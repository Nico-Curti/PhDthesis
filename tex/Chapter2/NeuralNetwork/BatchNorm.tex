\documentclass{standalone}

\begin{document}


\subsection[BatchNorm function]{BatchNorm function}\label{NN:batchnorm}

A common practice before the training of a Neural Network model is to apply some preprocessing to the input patterns.
A classical example is the normalization of training set, i.e it resembles a normal distribution with zero mean and unitary variance.
The initial preprocessing is useful to prevent the early saturation of non-linear activation functions (see section~\ref{NN:activation}).
Moreover, in this way we can ensure that all inputs are in the same range of values.

In a deep Neural Network architecture we can find the same problem also into the intermediate layers, because the distribution of the activations constantly changes during training.
This behavior produces a slowdown in the training convergence because each layer has to adapt itself to a new distribution of data in every training step (or \emph{epoch}).
This problem is also called \emph{internal covariate shift}.

A second problem arises from the heterogeneity of available input data.
If we tune the model parameters according to a given set of data, which inevitably will be limited, we can meet problems during the generalization, i.e the validation of our model using new data, to new samples if they belong to an equivalent, but deformed, distribution: this kind of problem passes under the name of \emph{over-fitting}.
A classical example is given by the image detection: if we train a Neural Network model using gray-scale images we can find generalization issues using colored images.
This problem can be solve using regularization techniques.

BatchNorm function (Batch Normalization) allows to overcome these problems with a continuous rescaling of the Neural Network intermediate values during the training\footnote{
  The input data to feed the Neural Network model are commonly packed into a series of \emph{batches}, i.e small subsets of data.
  The BatchNorm function takes its name from this nomenclature and it processes each batch independently.
}~\cite{Sergey2015BatchNorm}.
In this way we can ensure more stability to the extracted features~\cite{Lecun2000EffBackProp} during the training and a faster convergence.

In particular, the method processes the input of a given layer in order to fight the internal covariate shift problem removing the batch mean, normalizing by the batch variance:

$$
\mu_B = \frac{1}{m}\sum_{i=1}^{m}x_i \quad\quad {\sigma_B}^2 = \frac{1}{m-1}\sum_{i=1}^{m}(x_i - \mu_B)^2
$$
\\
where $m$ represents the batch-size and $x_i$ is the value of the pixel $x$ in the $i$-th image of the batch ($\in[0, m]$).
Thus, the input data becomes:

$$
\hat{x_i} = \frac{x_i - \mu_B}{\sqrt{{\sigma_B}^2 + \epsilon}}
$$
\\
where we add an extra $\epsilon$ in the denominator for numerical stability\footnote{
  Floating point numbers into a computer have finite precision and the variance can underflow bringing to infinite values in the BatchNorm equation.
}.
After this common rescaling, we apply a shift-scaling to the previous results:

$$
y_i = \gamma\hat{x_i} + \beta
$$
\\
where the $\gamma$ and $\beta$ coefficients are left as variables to be tuned during the training (they are learned during training).
The updating rule of the function parameters ($\gamma$ and $\beta$) is given by the derivative of the previous functions:

$$
\delta\beta = \sum_{i=1}^{m}{\delta_i}^l \quad\quad \delta\gamma = \sum_{i=0}^{m} {\delta_i}^l \cdot \mu_B
$$
\\
where $\delta^l$ is the error passed from the next layer of the network structure.
To complete the error propagation, we have to compute the derivative of the BatchNorm function output:

$$
{\delta_i}^{l-1} = \frac{{m}\cdot \delta\hat{x_i} - \sum_{j=1}^{m}\delta\hat{x_i} - \hat{x_i} \cdot \sum_{j=1}^{m} \delta\hat{x_i} \cdot \hat{x_i}}{m \cdot \sqrt{{\sigma_B}^2 + \epsilon} }
$$

Since the BatchNorm function is became a sort of standard into deep learning models, an efficient implementation of this algorithm is essential to achieve the best computational performances.
We have to take into account that batch-normalization procedure is commonly performed after a fully-connected layer or a convolutional one: the best performances are obtained merging the two functionality as much as possible, as suggested in~\cite{AlexeyAB}.

The \textsf{Byron} library is inspired by the \textsf{darknet} library provided by Redmon J. et al. and by its many branches.
Despite in each implementation we can find the BatchNorm function, aware of the author, in any version we can find a right implementation of this function as standalone method.
We have already highlighted that this normalization function can be efficiently joined to other function to increase the computational performances, but in these case we have to differentially manage the dimensions of the involved arrays.
A standalone implementation of the BatchNorm function required a rearrangement of its functions and it has provided into our \textsf{Byron} and \textsf{NumPyNet} libraries.
This was one of the various improvements provided by \textsf{Byron} against other \textsf{darknet}-like libraries.

Other common regularization techniques are given by the regularization of neuron outputs with penalty loss functions.
Classical examples are given by L1 (Laplacian) and L2 (Gaussian) penalties.
Both these functions are implemented either in \textsf{NumPyNet} and \textsf{Byron}, but for sake of brevity we will not discuss about them.

\end{document}
