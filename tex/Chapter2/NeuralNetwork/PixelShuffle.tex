\documentclass{standalone}

\begin{document}

% https://medium.com/@hirotoschwert/introduction-to-deep-super-resolution-c052d84ce8cf

\section[Pixel Shuffle]{Pixel Shuffle}\label{shuffler}

Pixel Shuffle layer is one of the most recent layer type introduced in modern deep learning Neural Network.
Its application is closely related to the single-image super-resolution (SISR) research, i.e the techniques ensemble which aim at restoring a high-resolution image from a single low-resolution one (see section~\ref{sr} for further details).

The first SISR neural networks start with a pre-processing of low-resolution image in input with a bicubic up-sample.
Then the image, with the same dimension of the desired output, feeds the model which aim to increase the resolution and fix its details.
In this way the amount of parameters and moreover the computational required by the training section increase (by a factor equal to the square of the desired up-sample scale), despite the image processing required is smaller.
To overcome this problem a Pixel Shuffle transformation, also known as \emph{sub-pixel convolution}, was introduced~\cite{Wenzhe2016Shuffle}.
The Pixel Shuffle transformation reorganize the low-resolution image channels to obtain a bigger image with few channels.
An example of this transformation is shown in Fig.~\ref{fig:pixel_shuffle}.

\begin{figure}[htbp]
\centering
\def\svgwidth{0.7\textwidth}
\input{./img/pixel_shuffle.pdf_tex}
\caption{Pixel Shuffle transformation.
On the left the input image with $scale^2$ (:= 9) channels.
On the right the result of Pixel Shuffle transformation.
Since the number of channels is perfect square the output is a single channel image with the rearrangement of the original ones.
}
\label{fig:pixel_shuffle}
\end{figure}

Pixel Shuffle rearranges the elements of the input tensor expressed as $H \times W \times C^2$ to form a $scale \cdot H \times scale \cdot W \times C$ tensor.
This can be very useful after a convolutional process, in which the number of filters chosen drastically increase the number of channels, to \quotes{invert} the transformation like a sort of \emph{deconvolution} function.

The main gain in using this transformation is the increment of computational efficiency of Neural Network model.
The introduction of Pixel Shuffle transformation in the Neural Network tail, i.e after a sequence of small processing steps which increase the number of features, we can reorganize the set of features into a single bigger image, i.e the desired output in a SISR application.
The feature processing steps, which generally are faced on with convolutional layers, can be used with smaller images in input and so can be performed faster since the upscaling task will be performed by a single Pixel Shuffle transformation.

Despite this transformation is became a standard in super-resolution applications and thus it can be found into the most common deep learning libraries (e.g \emph{Pytorch} and \emph{Tensorflow}) a C++ implementation is hard to find.
Moreover each library implements the transformation following its own data organization\footnote{
  The main difference between \emph{Pytorch} and \emph{Tensorflow} is related to the storage organization of the image.
  \emph{Tensorflow} has a \quotes{standard} input assessment as $H \times W \times C$.
  \emph{Pytorch} has a so-called channel-first implementation and so the input tensor is organized as $C \times H \times W$.
}.
For this reason we will propose in our libraries a dynamic version of the algorithm in C++ able to perform both version of the algorithm.

\end{document}
